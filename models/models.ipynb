{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c388da4c58ad46bf8d2d2cb79148937d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a983be45664048b42fffda93af0282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f48133f23dc44c9a9be7b200fe2eaaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28344a73c9e54a77958c1b7ddfc269b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aw82/miniconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f64ae4db0d4d93b1d7612c357b4f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b8d65b3242408aaf5359174b9ffd08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_by_layer = dict()\n",
    "for layer in range(32):\n",
    "    # self attn layers\n",
    "    model_weights_by_layer[f\"{layer}_self_attn_q_proj\"] = model.model.layers[layer].self_attn.q_proj.weight.data.cpu().numpy()\n",
    "    model_weights_by_layer[f\"{layer}_self_attn_k_proj\"] = model.model.layers[layer].self_attn.k_proj.weight.data.cpu().numpy()\n",
    "    model_weights_by_layer[f\"{layer}_self_attn_v_proj\"] = model.model.layers[layer].self_attn.v_proj.weight.data.cpu().numpy()\n",
    "    model_weights_by_layer[f\"{layer}_self_attn_o_proj\"] = model.model.layers[layer].self_attn.o_proj.weight.data.cpu().numpy()\n",
    "    # mlp layers\n",
    "    model_weights_by_layer[f\"{layer}_mlp_gate_proj\"] = model.model.layers[layer].mlp.gate_proj.weight.data.cpu().numpy()\n",
    "    model_weights_by_layer[f\"{layer}_mlp_up_proj\"] = model.model.layers[layer].mlp.up_proj.weight.data.cpu().numpy()\n",
    "    model_weights_by_layer[f\"{layer}_mlp_down_proj\"] = model.model.layers[layer].mlp.down_proj.weight.data.cpu().numpy()\n",
    "    \n",
    "model_weights_by_layer[\"embed_tokens\"] = model.model.embed_tokens.weight.data.cpu().numpy()\n",
    "model_weights_by_layer[\"lm_head\"] = model.lm_head.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"{model_name}.pickle\", 'wb') as handle:\n",
    "    pickle.dump(model_weights_by_layer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
